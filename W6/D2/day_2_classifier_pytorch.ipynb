{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bca85f41",
   "metadata": {},
   "source": [
    "# Day 2 - Classifying Embeddings with PyTorch and the Gemini API\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the Generative AI Course. In this notebook, you'll learn to use the embeddings produced by the Gemini API to train a model that can classify newsgroup posts into their categories (the newsgroup itself) from the post contents.\n",
    "\n",
    "This technique uses the Gemini API's embeddings as input, avoiding the need to train on text input directly, and as a result it is able to perform quite well using relatively few examples.\n",
    "\n",
    "**Prerequisites**:\n",
    "- You need a Google Cloud Project with the Gemini API enabled.\n",
    "- You need an API key stored in the `GOOGLE_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d136b6e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "pip install -U -q \"google-genai\" scikit-learn torch tqdm pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bbba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import email\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.api_core import retry\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931d66f",
   "metadata": {},
   "source": [
    "### Set up your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300bf036",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Please set the GOOGLE_API_KEY environment variable.\")\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60c700",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The [20 Newsgroups Text Dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) contains 18,000 newsgroups posts on 20 topics divided into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245dab8e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset=\"train\")\n",
    "newsgroups_test = fetch_20newsgroups(subset=\"test\")\n",
    "\n",
    "# View list of class names for dataset\n",
    "print(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86cae25",
   "metadata": {},
   "source": [
    "Start by preprocessing the data. To remove any sensitive information like names and email addresses, you will take only the subject and body of each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e086b01",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_newsgroup_row(data):\n",
    "    # Extract only the subject and body\n",
    "    msg = email.message_from_string(data)\n",
    "    text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"\n",
    "    # Strip any remaining email addresses\n",
    "    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n",
    "    # Truncate each entry to 5,000 characters\n",
    "    text = text[:5000]\n",
    "    return text\n",
    "\n",
    "def preprocess_newsgroup_data(newsgroup_dataset):\n",
    "    # Put data points into dataframe\n",
    "    df = pd.DataFrame(\n",
    "        {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}\n",
    "    )\n",
    "    # Clean up the text\n",
    "    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n",
    "    # Match label to target name index\n",
    "    df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply preprocessing function to training and test datasets\n",
    "df_train = preprocess_newsgroup_data(newsgroups_train)\n",
    "df_test = preprocess_newsgroup_data(newsgroups_test)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae2aab",
   "metadata": {},
   "source": [
    "Next, you will sample some of the data by taking 100 data points in the training dataset, and dropping a few of the categories to run through this tutorial. Choose the science categories to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca023d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(df, num_samples, classes_to_keep):\n",
    "    # Sample rows, selecting num_samples of each Label.\n",
    "    df = (\n",
    "        df.groupby(\"Label\")[df.columns]\n",
    "        .apply(lambda x: x.sample(num_samples))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n",
    "\n",
    "    # We have fewer categories now, so re-calibrate the label encoding.\n",
    "    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n",
    "    df[\"Encoded Label\"] = df[\"Class Name\"].cat.codes\n",
    "\n",
    "    return df\n",
    "\n",
    "TRAIN_NUM_SAMPLES = 100\n",
    "TEST_NUM_SAMPLES = 25\n",
    "CLASSES_TO_KEEP = \"sci\"\n",
    "\n",
    "df_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\n",
    "df_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)\n",
    "\n",
    "print(df_train.value_counts(\"Class Name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd5127",
   "metadata": {},
   "source": [
    "## Create the embeddings\n",
    "\n",
    "In this section, you will generate embeddings for each piece of text using the Gemini API embeddings endpoint.\n",
    "\n",
    "We will use `task_type=\"classification\"` for the training embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper to retry when per-minute quota is reached.\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "@retry.Retry(predicate=is_retriable, timeout=300.0)\n",
    "def embed_fn(text: str) -> list[float]:\n",
    "    response = client.models.embed_content(\n",
    "        model=\"models/text-embedding-004\",\n",
    "        contents=text,\n",
    "        config=types.EmbedContentConfig(\n",
    "            task_type=\"classification\",\n",
    "        ),\n",
    "    )\n",
    "    return response.embeddings[0].values\n",
    "\n",
    "def create_embeddings(df):\n",
    "    tqdm.tqdm.pandas()\n",
    "    df[\"Embeddings\"] = df[\"Text\"].progress_apply(embed_fn)\n",
    "    return df\n",
    "\n",
    "print(\"Generating embeddings for Training set...\")\n",
    "df_train = create_embeddings(df_train)\n",
    "print(\"Generating embeddings for Test set...\")\n",
    "df_test = create_embeddings(df_test)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3243c8",
   "metadata": {},
   "source": [
    "## Build a classification model (PyTorch)\n",
    "\n",
    "Here you will define a simple model using PyTorch that accepts the embedding data as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch\n",
    "X_train = np.stack(df_train[\"Embeddings\"].values)\n",
    "y_train = df_train[\"Encoded Label\"].values\n",
    "X_test = np.stack(df_test[\"Embeddings\"].values)\n",
    "y_test = df_test[\"Encoded Label\"].values\n",
    "\n",
    "# Convert to Tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the Model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(input_size, num_classes)\n",
    "        # Note: CrossEntropyLoss includes Softmax, so we output raw logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "embedding_size = len(df_train[\"Embeddings\"].iloc[0])\n",
    "num_classes = len(df_train[\"Class Name\"].unique())\n",
    "\n",
    "model = Classifier(embedding_size, num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d9731",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32725488",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Print stats every 5 epochs or so\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c2fc3",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9671259",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c94b8",
   "metadata": {},
   "source": [
    "## Try a custom prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(text: str) -> torch.Tensor:\n",
    "    \"\"\"Infer categories from the provided text.\"\"\"\n",
    "    # Calculate embedding\n",
    "    embedded_val = embed_fn(text)\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    inp = torch.tensor(np.array([embedded_val]), dtype=torch.float32)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inp)\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        \n",
    "    return probs[0]\n",
    "\n",
    "new_text = \"\"\"\n",
    "First-timer looking to get out of here.\n",
    "\n",
    "Hi, I'm writing about my interest in travelling to the outer limits!\n",
    "\n",
    "What kind of craft can I buy? What is easiest to access from this 3rd rock?\n",
    "\n",
    "Let me know how to do that please.\n",
    "\"\"\"\n",
    "\n",
    "result = make_prediction(new_text)\n",
    "\n",
    "# Get categories from the dataframe\n",
    "categories = df_train[\"Class Name\"].cat.categories\n",
    "\n",
    "for idx, category in enumerate(categories):\n",
    "    print(f\"{category}: {result[idx] * 100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a789270d",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "To explore training custom models with PyTorch further, check out the [PyTorch tutorials](https://pytorch.org/tutorials/)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
