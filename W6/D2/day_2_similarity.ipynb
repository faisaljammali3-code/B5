{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b206461b",
   "metadata": {},
   "source": [
    "# Day 2 - Embeddings and Similarity Scores\n",
    "\n",
    "Welcome to the Generative AI Course!\n",
    "\n",
    "In this tutorial, you will use the Gemini API's embedding endpoint to explore similarity scores.\n",
    "\n",
    "**Prerequisites**:\n",
    "- You need a Google Cloud Project with the Gemini API enabled.\n",
    "- You need an API key stored in the `GOOGLE_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d4ec9",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, we'll install the necessary libraries.\n",
    "\n",
    "```bash\n",
    "pip install -U -q \"google-genai\" pandas seaborn matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5dfcb",
   "metadata": {},
   "source": [
    "### Set up your API key\n",
    "Ensure your `GOOGLE_API_KEY` is set in your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d146914",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Please set the GOOGLE_API_KEY environment variable.\")\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56def43a",
   "metadata": {},
   "source": [
    "## Calculate similarity scores\n",
    "\n",
    "This example embeds some variations on the pangram, `The quick brown fox jumps over the lazy dog`, including spelling mistakes and shortenings of the phrase. Another pangram and a somewhat unrelated phrase have been included for comparison.\n",
    "\n",
    "In this task, you are going to use the embeddings to calculate similarity scores, so the `task_type` for these embeddings is `semantic_similarity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01b0ed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'The quick brown fox jumps over the lazy dog.',\n",
    "    'The quick rbown fox jumps over the lazy dog.',\n",
    "    'teh fast fox jumps over the slow woofer.',\n",
    "    'a quick brown fox jmps over lazy dog.',\n",
    "    'brown fox jumping over dog',\n",
    "    'fox > dog',\n",
    "    # Alternative pangram for comparison:\n",
    "    'The five boxing wizards jump quickly.',\n",
    "    # Unrelated text, also for comparison:\n",
    "    'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus et hendrerit massa. Sed pulvinar, nisi a lobortis sagittis, neque risus gravida dolor, in porta dui odio vel purus.',\n",
    "]\n",
    "\n",
    "response = client.models.embed_content(\n",
    "    model='models/text-embedding-004',\n",
    "    contents=texts,\n",
    "    config=types.EmbedContentConfig(task_type='semantic_similarity')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd065ff",
   "metadata": {},
   "source": [
    "Define a short helper function that will make it easier to display longer embedding texts in our visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd11ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(t: str, limit: int = 50) -> str:\n",
    "    \"\"\"Truncate labels to fit on the chart.\"\"\"\n",
    "    if len(t) > limit:\n",
    "        return t[:limit-3] + '...'\n",
    "    else:\n",
    "        return t\n",
    "\n",
    "truncated_texts = [truncate(t) for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f40dc0f",
   "metadata": {},
   "source": [
    "A similarity score of two embedding vectors can be obtained by calculating their inner product. If $\\mathbf{u}$ is the first embedding vector, and $\\\\mathbf{v}$ the second, this is $\\mathbf{u}^T \\\\mathbf{v}$. As the API provides embedding vectors that are normalised to unit length, this is also the cosine similarity.\n",
    "\n",
    "This score can be computed across all embeddings through the matrix self-multiplication: `df @ df.T`.\n",
    "\n",
    "Note that the range from 0.0 (completely dissimilar) to 1.0 (completely similar) is depicted in the heatmap from light (0.0) to dark (1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b41daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the embeddings in a dataframe.\n",
    "df = pd.DataFrame([e.values for e in response.embeddings], index=truncated_texts)\n",
    "\n",
    "# Perform the similarity calculation\n",
    "sim = df @ df.T\n",
    "\n",
    "# Draw!\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sim, vmin=0, vmax=1, cmap=\"Greens\", annot=True)\n",
    "plt.title(\"Semantic Similarity Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc3030",
   "metadata": {},
   "source": [
    "You can see the scores for a particular term directly by looking it up in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sim['The quick brown fox jumps over the lazy dog.'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc6e7b",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* Explore [search re-ranking using embeddings](https://github.com/google-gemini/cookbook/blob/main/examples/Search_reranking_using_embeddings.ipynb) with the Wikipedia API\n",
    "* Perform [anomaly detection using embeddings](https://github.com/google-gemini/cookbook/blob/main/examples/Anomaly_detection_with_embeddings.ipynb)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
