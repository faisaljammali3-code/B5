{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f855f631",
   "metadata": {},
   "source": [
    "# Arabic Named-Entity Recognition (NER) — Assignment\n",
    "\n",
    "This notebook guides you through building an Arabic NER model using the ANERCorp dataset (`asas-ai/ANERCorp`). Fill in the TODO cells to complete the exercise.\n",
    "\n",
    "- **Objective:** Train a token-classification model (NER) that labels tokens with entity tags (e.g., people, locations, organizations).\n",
    "- **Dataset:** `asas-ai/ANERCorp` — contains tokenized Arabic text and tag sequences.\n",
    "- **Typical Labels:** `B-PER`, `I-PER` (person), `B-LOC`, `I-LOC` (location), `B-ORG`, `I-ORG` (organization), and `O` (outside/no entity). Your code should extract the exact label set from the dataset and build `label_list`, `id2label`, and `label2id` mappings.\n",
    "- **Key Steps (what you will implement):**\n",
    "  1. Load the dataset and inspect samples.\n",
    "  2. Convert the provided words into sentence groupings (use `.` `?` `!` as sentence delimiters) before tokenization so sentence boundaries are preserved.\n",
    "  3. Tokenize with a pretrained Arabic tokenizer and align tokenized sub-words with original labels (use `-100` for tokens to ignore in loss).\n",
    "  4. Prepare `tokenized_datasets` and data collator for dynamic padding.\n",
    "  5. Configure and run model training using `AutoModelForTokenClassification` and `Trainer`.\n",
    "  6. Evaluate using `seqeval` (report precision, recall, F1, and accuracy) and run inference with a pipeline.\n",
    "\n",
    "- **Evaluation:** Use the `seqeval` metric (entity-level precision, recall, F1). When aligning predictions and labels, filter out `-100` entries so only real token labels are compared.\n",
    "\n",
    "- **Deliverables:** Completed notebook with working cells for data loading, tokenization/label alignment, training, evaluation, and an inference example. Add short comments explaining choices (e.g., sentence-splitting strategy, tokenizer settings).\n",
    "\n",
    "Good luck — implement each TODO in order and run the cells to verify output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b101905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# TODO: Install the required packages for Arabic NER with transformers\n",
    "# Required packages: transformers, datasets, seqeval, evaluate, accelerate\n",
    "# Use pip install with -q flag to suppress output\n",
    "\n",
    "!pip install transformers datasets seqeval evaluate accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abb573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.config', 'sample_data']\n"
     ]
    }
   ],
   "source": [
    "# TODO: List the files in the current directory to explore the workspace\n",
    "# Hint: Use a simple command to display directory contents\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import os\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da23007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8d27de82b74f56beb1ef75ea2a2470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe1d351a7624deab97c94047f303224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-83c5047e14e689(…):   0%|          | 0.00/855k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de54afc9f7748e99d069c925857c636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001-245173671c05c71(…):   0%|          | 0.00/175k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f3ea7ec1ee41a282e7a5dfc30e9e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/125102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38c1dcbef0a4ac4a29083545b43cd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Split: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word', 'tag'],\n",
      "        num_rows: 125102\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word', 'tag'],\n",
      "        num_rows: 25008\n",
      "    })\n",
      "})\n",
      "Sample Entry: {'word': 'فرانكفورت', 'tag': 'B-LOC'}\n",
      "\n",
      "Label List: ['B-LOC', 'B-MISC', 'B-ORG', 'B-PERS', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PERS', 'O']\n",
      "id2label: {0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'B-PERS', 4: 'I-LOC', 5: 'I-MISC', 6: 'I-ORG', 7: 'I-PERS', 8: 'O'}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load the ANERCorp dataset and extract label mappings\n",
    "# Steps:\n",
    "# 1. Import required libraries (datasets, numpy)\n",
    "# 2. Load the \"asas-ai/ANERCorp\" dataset using load_dataset()\n",
    "# 3. Inspect the dataset structure - print the splits and a sample entry\n",
    "# 4. Extract unique tags from the training split\n",
    "# 5. Create label_list (sorted), id2label, and label2id mappings\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"asas-ai/ANERCorp\")\n",
    "\n",
    "print(f\"Dataset Split: {dataset}\")\n",
    "print(f\"Sample Entry: {dataset['train'][0]}\")\n",
    "\n",
    "unique_tags = set(dataset['train']['tag'])\n",
    "\n",
    "label_list = sorted(list(unique_tags))\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "print(f\"\\nLabel List: {label_list}\")\n",
    "print(f\"id2label: {id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6aa5b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data (First 10 rows):\n",
      "        word    tag\n",
      "0  فرانكفورت  B-LOC\n",
      "1         (د      O\n",
      "2          ب      O\n",
      "3         أ)      O\n",
      "4       أعلن      O\n",
      "5      اتحاد  B-ORG\n",
      "6      صناعة  I-ORG\n",
      "7   السيارات  I-ORG\n",
      "8         في      O\n",
      "9    ألمانيا  B-LOC\n",
      "\n",
      "Dataset Features:\n",
      "{'word': Value('string'), 'tag': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Verify the dataset was loaded correctly\n",
    "# Print the dataframe or dataset summary to inspect the data structure\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import pandas as pd\n",
    "print(\"Sample Data (First 10 rows):\")\n",
    "df_sample = pd.DataFrame(dataset['train'][:10])\n",
    "print(df_sample)\n",
    "\n",
    "# Print the features summary again\n",
    "print(\"\\nDataset Features:\")\n",
    "print(dataset['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be78b0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2b6a13a6b747e8ad8507bd253ba179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/381 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d0026cce6d4d598a4307ba82caf8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a434daf6b2c4acc853010e3cecfb90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15462464d74644fb818920eca84d898d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70122856b2446cab9bcaecbf0ecb5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping words into sentences... this may take a moment.\n",
      "Original Row Count (Words): 125102\n",
      "New Row Count (Sentences): 4262\n",
      "Sample Sentence: ['فرانكفورت', '(د', 'ب', 'أ)', 'أعلن', 'اتحاد', 'صناعة', 'السيارات', 'في', 'ألمانيا', 'امس', 'الاول', 'أن', 'شركات', 'صناعة', 'السيارات', 'في', 'ألمانيا', 'تواجه', 'عاما', 'صعبا', 'في', 'ظل', 'ركود', 'السوق', 'الداخلية', 'والصادرات', 'وهي', 'تسعي', 'لان', 'يبلغ', 'الانتاج', 'حوالي', 'خمسة', 'ملايين', 'سيارة', 'في', 'عام', '2002', '.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407f0d6296084c9893f647b86f75d0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7305d37e5b94cabaa5d736a2ca054ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization Complete!\n",
      "Input IDs shape: 46\n",
      "Labels shape: 46\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load tokenizer and create tokenization function\n",
    "# Steps:\n",
    "# 1. Import AutoTokenizer from transformers\n",
    "# 2. Set model_checkpoint to \"aubmindlab/bert-base-arabertv02\"\n",
    "# 3. Load the tokenizer using AutoTokenizer.from_pretrained()\n",
    "# 4. Create tokenize_and_align_labels function that:\n",
    "#    - Tokenizes the input text (is_split_into_words=True)\n",
    "#    - Maps tokens to their original words\n",
    "#    - Handles special tokens by setting them to -100\n",
    "#    - Aligns labels with sub-word tokens\n",
    "#    - Returns tokenized inputs with labels\n",
    "# 5. Important: Convert words to sentences using punctuation marks \".?!\" as sentence delimiters\n",
    "#    - This helps the model understand sentence boundaries\n",
    "#    - Hint (suggested approach): group `examples['word']` into sentence lists using \".?!\" as end markers, e.g.:\n",
    "#        sentences = []\n",
    "#        current = []\n",
    "#        for w in examples['word']:\n",
    "#            current.append(w)\n",
    "#            if w in ['.', '?', '!'] or (len(w) > 0 and w[-1] in '.?!'):\n",
    "#                sentences.append(current)\n",
    "#                current = []\n",
    "#        if current:\n",
    "#            sentences.append(current)\n",
    "#      Then align `examples['tag']` accordingly to these sentence groups before tokenization.\n",
    "# 6. Apply the function to the entire dataset using dataset.map()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "model_checkpoint = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "def group_sentences(dataset_split):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "    \n",
    "    for word, tag in zip(dataset_split['word'], dataset_split['tag']):\n",
    "        current_sentence.append(word)\n",
    "        current_labels.append(label2id[tag]) # Convert tag string to ID here\n",
    "        \n",
    "        if word in ['.', '?', '!'] or (isinstance(word, str) and word.endswith(('.', '?', '!'))):\n",
    "            sentences.append(current_sentence)\n",
    "            labels.append(current_labels)\n",
    "            current_sentence = []\n",
    "            current_labels = []\n",
    "            \n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        labels.append(current_labels)\n",
    "        \n",
    "    return {\"tokens\": sentences, \"ner_tags\": labels}\n",
    "\n",
    "print(\"Grouping words into sentences... this may take a moment.\")\n",
    "grouped_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_dict(group_sentences(dataset['train'])),\n",
    "    \"test\": Dataset.from_dict(group_sentences(dataset['test']))\n",
    "})\n",
    "\n",
    "print(f\"Original Row Count (Words): {len(dataset['train'])}\")\n",
    "print(f\"New Row Count (Sentences): {len(grouped_datasets['train'])}\")\n",
    "print(f\"Sample Sentence: {grouped_datasets['train'][0]['tokens']}\")\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = grouped_datasets.map(\n",
    "    tokenize_and_align_labels, \n",
    "    batched=True,\n",
    "    remove_columns=grouped_datasets['train'].column_names \n",
    ")\n",
    "\n",
    "print(\"\\nTokenization Complete!\")\n",
    "print(\"Input IDs shape:\", len(tokenized_datasets['train'][0]['input_ids']))\n",
    "print(\"Labels shape:\", len(tokenized_datasets['train'][0]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e5b09f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186ed9749ac44586bd11d5bc0616fb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Define the compute_metrics function for model evaluation\n",
    "# Steps:\n",
    "# 1. Import evaluate and load \"seqeval\" metric\n",
    "# 2. Create compute_metrics function that:\n",
    "#    - Extracts predictions from model outputs using argmax\n",
    "#    - Filters out -100 labels (special tokens and sub-words)\n",
    "#    - Converts prediction and label IDs back to label names\n",
    "#    - Computes seqeval metrics (precision, recall, f1, accuracy)\n",
    "#    - Returns results as a dictionary\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    \n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e6e403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1b031e0f5d44d59a1bf9b52ddc25b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-1847859829.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='801' max='801' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [801/801 04:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.161721</td>\n",
       "      <td>0.826860</td>\n",
       "      <td>0.773304</td>\n",
       "      <td>0.799186</td>\n",
       "      <td>0.963092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.144664</td>\n",
       "      <td>0.816157</td>\n",
       "      <td>0.817943</td>\n",
       "      <td>0.817049</td>\n",
       "      <td>0.969250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.147613</td>\n",
       "      <td>0.818582</td>\n",
       "      <td>0.813567</td>\n",
       "      <td>0.816067</td>\n",
       "      <td>0.968930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=801, training_loss=0.08560325560647153, metrics={'train_runtime': 249.1531, 'train_samples_per_second': 51.318, 'train_steps_per_second': 3.215, 'total_flos': 592339052900340.0, 'train_loss': 0.08560325560647153, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the model and configure training\n",
    "# Steps:\n",
    "# 1. Import AutoModelForTokenClassification, TrainingArguments, Trainer, and DataCollatorForTokenClassification\n",
    "# 2. Load the model using AutoModelForTokenClassification.from_pretrained() with:\n",
    "#    - model_checkpoint\n",
    "#    - num_labels based on label_list length\n",
    "#    - id2label and label2id mappings\n",
    "# 3. Create TrainingArguments with:\n",
    "#    - output directory \"arabert-ner\"\n",
    "#    - evaluation_strategy=\"epoch\"\n",
    "#    - learning_rate=2e-5\n",
    "#    - batch_size=16 (both train and eval)\n",
    "#    - num_train_epochs=3\n",
    "#    - weight_decay=0.01\n",
    "# 4. Create a DataCollatorForTokenClassification for dynamic padding\n",
    "# 5. Initialize the Trainer with model, args, datasets, data_collator, tokenizer, and compute_metrics\n",
    "# 6. Call trainer.train() to start training\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"arabert-ner\",\n",
    "    eval_strategy=\"epoch\",      \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "496b7ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: أعلن المدير التنفيذي لشركة أبل تيم كوك عن افتتاح فرع جديد في الرياض.\n",
      "\n",
      "--- Extracted Entities ---\n",
      "Entity: أبل          | Label: ORG    | Score: 0.97\n",
      "Entity: تيم كوك      | Label: PERS   | Score: 0.99\n",
      "Entity: الرياض       | Label: LOC    | Score: 0.98\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test the trained model with inference\n",
    "# Steps:\n",
    "# 1. Import pipeline from transformers\n",
    "# 2. Create an NER pipeline using the trained model and tokenizer\n",
    "# 3. Use aggregation_strategy=\"simple\" to merge sub-tokens back into words\n",
    "# 4. Test the pipeline with an Arabic text sample\n",
    "# 5. Pretty print the results showing entity, label, and confidence score\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0 \n",
    ")\n",
    "\n",
    "text = \"أعلن المدير التنفيذي لشركة أبل تيم كوك عن افتتاح فرع جديد في الرياض.\"\n",
    "print(f\"Original Text: {text}\\n\")\n",
    "\n",
    "results = ner_pipeline(text)\n",
    "\n",
    "print(\"--- Extracted Entities ---\")\n",
    "for entity in results:\n",
    "\n",
    "    print(f\"Entity: {entity['word']:<12} | Label: {entity['entity_group']:<6} | Score: {entity['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
