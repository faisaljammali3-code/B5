{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic 100k Reviews (Part 1): Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is the first part of a comprehensive end-to-end **Natural Language Processing (NLP) pipeline** for Arabic text classification. In this part, we focus on the foundational steps that transform raw Arabic text data into a clean, analyzed dataset ready for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Acquire and extract text data** from external sources (Kaggle datasets)\n",
    "2. **Apply Arabic-specific preprocessing techniques** including:\n",
    "   - Text cleaning (removing URLs, handles, hashtags)\n",
    "   - Normalization (removing Arabic elongations and diacritics)\n",
    "   - Tokenization (breaking text into tokens)\n",
    "   - Stemming (reducing words to root forms)\n",
    "   - Stop word removal\n",
    "3. **Understand the importance of preprocessing order** and why it matters\n",
    "4. **Perform Exploratory Data Analysis (EDA)** on text data to understand:\n",
    "   - Class distribution\n",
    "   - Text length distributions\n",
    "   - Language detection\n",
    "   - Vocabulary analysis\n",
    "   - Word frequency patterns by class\n",
    "5. **Recognize preprocessing trade-offs** (speed vs. accuracy, information loss vs. efficiency)\n",
    "6. **Use appropriate tools** for Arabic NLP (PyArabic, NLTK, Qalsadi, CAMeL Tools)\n",
    "\n",
    "An **NLP pipeline** (as introduced in Module 1) is a systematic sequence of steps that transforms raw text into a format suitable for machine learning models.\n",
    "\n",
    "**Later, in Part 2:** the goal is to build a model that can classify Arabic reviews into **Positive** or **Negative** sentiment.\n",
    "\n",
    "---\n",
    "\n",
    "> In short, we learn: What to do, how to do it, why do it, and where (order).\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Setup and Imports** - Installing dependencies and importing libraries\n",
    "2. **Data Acquisition** - Downloading and extracting the Arabic 100k Reviews dataset from Kaggle\n",
    "3. **Data Loading** - Loading the dataset into a pandas DataFrame\n",
    "4. **Exploratory Data Analysis (EDA)**:\n",
    "   - Class distribution analysis\n",
    "   - Text length distributions\n",
    "   - Language detection\n",
    "   - Vocabulary analysis\n",
    "   - Word frequency patterns by class\n",
    "5. **Preprocessing Pipeline**:\n",
    "   - Text cleaning (URLs, handles, hashtags)\n",
    "   - Normalization (Arabic elongations, diacritics)\n",
    "   - Tokenization\n",
    "   - Stemming/Lemmatization\n",
    "   - Stop word removal\n",
    "6. **Preprocessing Order** - Understanding why order matters\n",
    "7. **Preprocessed Data** - Saving cleaned data for Part 2 (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [NLP Pipeline, Ali Alameer | GitHub](https://github.com/Ali-Alameer/NLP/blob/main/week2_pipeline_part1.ipynb)\n",
    "- [NLP_Getting_started(Preprocessing), Ali H. El-Kassas | Kaggle](https://www.kaggle.com/code/ali01lulu/03-nlp-getting-started-preprocessing/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary of Terms\n",
    "\n",
    "**Corpus** (plural: corpora): A collection of text documents used for linguistic analysis or training NLP models. In this lab, our corpus is the Arabic reviews dataset.\n",
    "\n",
    "**Tokenization**: The process of breaking text into smaller units called tokens (words, subwords, or characters). As you learned in Module 1, tokenization is a fundamental step in the NLP pipeline.\n",
    "\n",
    "**Stemming**: A rule-based technique that reduces words to their root or stem form by removing prefixes and suffixes. For example, \"running\", \"runs\", \"ran\" ‚Üí \"run\". Faster but less accurate than lemmatization.\n",
    "\n",
    "**Lemmatization**: A dictionary-based technique that converts words to their base form (lemma) using linguistic knowledge. More accurate than stemming but slower. For example, \"better\" ‚Üí \"good\" (stemming might not catch this).\n",
    "\n",
    "**Stop Words**: Common words that appear frequently but carry little semantic meaning (e.g., \"the\", \"is\", \"at\" in English; \"ŸÅŸä\", \"ŸÖŸÜ\", \"ÿ•ŸÑŸâ\" in Arabic). Removing them can reduce vocabulary size and computational overhead.\n",
    "\n",
    "**Normalization**: The process of converting text to a standard form. This includes case folding (lowercase), removing diacritics (in Arabic), and handling elongations.\n",
    "\n",
    "**Vocabulary**: The set of unique tokens (words) in a corpus. Building a vocabulary is essential for converting text to numerical representations.\n",
    "\n",
    "**Data Leakage**: When information from the test set inadvertently influences the training process. We prevent this by splitting data before preprocessing.\n",
    "\n",
    "**Exploratory Data Analysis (EDA)**: The process of analyzing datasets to summarize their main characteristics, often using visual methods. In NLP, this includes examining class distributions, text lengths, and word frequencies.\n",
    "\n",
    "**Preprocessing Pipeline**: A sequence of preprocessing steps applied in a specific order. The order matters because later steps depend on earlier ones.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We begin by installing necessary packages and importing all required libraries. Grouping imports at the top makes dependencies clear and follows Python best practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "We group imports by category:\n",
    "\n",
    "1. **Standard Library**: Python built-in modules\n",
    "2. **Data Science**: NumPy, Pandas for data manipulation\n",
    "3. **Machine Learning**: Scikit-learn for data splitting and preprocessing\n",
    "4. **Visualization**: For EDA plots\n",
    "5. **NLP Libraries**: Arabic-specific and general NLP tools\n",
    "   - [**PyArabic**](https://github.com/linuxscout/pyarabic): Arabic text manipulation (removing diacritics, elongations)\n",
    "   - [**tnkeeh (ÿ™ŸÜŸÇŸäÿ≠)**](https://github.com/ARBML/tnkeeh) is an Arabic preprocessing library based on regex.\n",
    "   - [**nltk**](https://www.nltk.org/): Natural Language Toolkit (stop words, stemmers)\n",
    "   - [**qalsadi**](https://github.com/linuxscout/qalsadi): Arabic lemmatizer\n",
    "   - [**camel-tools**](https://github.com/CAMeL-Lab/camel_tools): CAMeL Tools for advanced Arabic NLP (morphological tokenization)\n",
    "   - [**ekphrasis**](https://github.com/cbaziotis/ekphrasis): Text processing tool for social media (tokenization, word normalization, word segmentation, spell correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Refer to the README.md for lab setup instructions\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import random\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from spellchecker import SpellChecker # https://pyspellchecker.readthedocs.io/en/latest/\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Arabic NLP libraries\n",
    "import pyarabic.araby as araby\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJlqWlyTtFd5"
   },
   "source": [
    "# Data acquisition\n",
    "\n",
    "The data is readily available on Kaggle, thanks to [Abed Khooli](https://www.kaggle.com/datasets/abedkhooli/arabic-100k-reviews) as a `.tsv` file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dSg2C4AtFd5"
   },
   "source": [
    "The dataset combines reviews from hotels, books, movies, products and a few airlines. It has three classes (Mixed, Negative and Positive). Most were mapped from reviewers' ratings with 3 being mixed, above 3 positive and below 3 negative. Each row has a label and text separated by a tab (tsv). Text (reviews) were cleaned by removing Arabic diacritics and non-Arabic characters. The dataset has no duplicate reviews.\n",
    "\n",
    "for more information, see [Dataset Card](https://www.kaggle.com/datasets/abedkhooli/arabic-100k-reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgZNFTCttFd5"
   },
   "source": [
    "1. Download the dataset from Kaggle: `abedkhooli/arabic-100k-reviews`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MdZnmBgtFd6",
    "outputId": "4e0fad0d-e977-4a07-b640-62e332837ee2"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d abedkhooli/arabic-100k-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Unzip the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0rY5CfxtFd6"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('arabic-100k-reviews.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuhVN2H2tFd6"
   },
   "source": [
    "## Data extraction\n",
    "\n",
    "The second step in the NLP pipeline is extracting the text from its native form (such as pdf, image, html, or csv).\n",
    "\n",
    "1. Load the data into a pandas DataFrame, knowing the separator is a tab (`\\t`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('ar_reviews_100k.tsv', delimiter='\\t')\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sample 10k examples only to speed things up\n",
    "   1. Note that we need to stratify the data to keep the same class distribution in the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jc2k_rptPRP"
   },
   "outputs": [],
   "source": [
    "# We will use only a subset of the data to speed things up\n",
    "df_raw, _ = train_test_split(\n",
    "    df_raw, train_size=10_000, random_state=SEED,\n",
    "    stratify=df_raw['label'], # keep the same class distribution\n",
    ")\n",
    "df_raw = df_raw.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNNw9PmPtFd7"
   },
   "source": [
    "- If we had our text in, say, HTML or JSON, we would need to use an **HTML parser** or **JSON parser** to extract the relevant text from it.\n",
    "- Extraction was straight-forward in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJTp9bpCtFd7"
   },
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fao4zaJ5tFd7"
   },
   "source": [
    "Let's show some text examples.\n",
    "\n",
    "Since we are dealing with **long texts**, we would need to increase the `max_colwidth` to show the full text in each cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gINsNSCtFd7"
   },
   "outputs": [],
   "source": [
    "# Show more of the text in each column\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using the `display` function to show the dataframe in a nice format. It is what happens when you put the dataframe in a cell by itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "u-gIm8QttFd7",
    "outputId": "21431a1c-1943-4b1c-aa43-6bf8b570c3ba"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Better than print: display shows the dataframe in a nice format\n",
    "# This is effectively what happens when you put the dataframe in a cell by itself\n",
    "print('first 5 rows:')\n",
    "display(df_raw.head())\n",
    "\n",
    "print('sample of 5 rows:')\n",
    "display(df_raw.sample(5))\n",
    "\n",
    "print('last 5 rows:')\n",
    "display(df_raw.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58HzC2UCtFd7"
   },
   "source": [
    "Observation: Some words include repeated words like in the following sentence\n",
    "```\n",
    "ŸÖÿ≥ŸÑÿ≥ŸÑ ŸÇÿØŸäŸÖ ...... ŸÖŸÑŸÑ .. ÿ≤ŸáŸÇ .. ÿ≠ÿ¥Ÿà ŸÉÿ™ŸäŸäŸäŸäŸäŸäŸäŸäŸäŸäŸäŸäŸäŸäÿ±. ŸÑŸäŸá ÿµŸÅÿ≠ÿ© ÿüÿü ... ŸÅŸä ÿ≠ŸäŸÜ ÿßŸÜ ÿßŸÑÿ≠ÿØ Ÿàÿ™ÿ© ÿØŸä ŸÖŸÖŸÉ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9ZKcBMstFd8"
   },
   "source": [
    "Observation: some reviews are in Modern Standard Arabic (MSA) and others are in colloquial dialect (ÿßŸÑŸÑŸáÿ¨ÿ© ÿßŸÑÿπÿßŸÖŸäÿ©)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJ1rKLrKtFd8"
   },
   "source": [
    "Question: what is the distribution of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "D14ikT--tFd8",
    "outputId": "5e86f7bd-dad1-4c0c-f6eb-7069c480fa08"
   },
   "outputs": [],
   "source": [
    "df_raw['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93irGZSMtFd8"
   },
   "source": [
    "Observation: equal distribution for all labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nloGQ4rStFd9"
   },
   "source": [
    "# Text cleaning & pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJRG5OQOtFd9"
   },
   "source": [
    "### Preprocessing depends on multiple factors:\n",
    "\n",
    "**Language**\n",
    "\n",
    "- English vs. Arabic\n",
    "- Different stemmer, lemmatizer, punctuation, digits, tashkeel, ...etc.\n",
    "\n",
    "**Data Source**\n",
    "- Tweets: you might need lots of spelling correction, and dealing with edge cases (slang, jargon, emojis, ..etc.)\n",
    "- Research papers: already proof-read, standard English, better grammer, ..etc.\n",
    "\n",
    "**Task**\n",
    "- Keyword-based search: you'd do search with and without stopword removal and combine results to give final answer.\n",
    "- Translation: everyword is important, can't remove stop-words because word-order and context matters.\n",
    "\n",
    "Remember, no matter how good your preprocessing is, **\"garbage in, garbage out\"**. That is, flawed, biased or poor quality (\"garbage\") input (training data) produces an output (model) of similar (\"garbage\") quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1WtU9ZbtFd9"
   },
   "source": [
    "**Order of preprocessing operations does matter**\n",
    "\n",
    "Example: if you clean by removing punctuations (`@`) you can't remove mentions or handles (`@blabla`) or URLs. You need first to remove handles then remove punctuations.\n",
    "\n",
    "Also, in English: words like `can't` has an apostrophe, which would be removed. Better: expand contractions: `can't -> cannot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNXGdwkGtFd9"
   },
   "source": [
    "### Cleaning Text\n",
    "\n",
    "- Remove or replace non-useful text (**depends on your end-goal**):\n",
    "    - extra whitespace: `\" \\r\\n\\t\"`\n",
    "    - numbers: `\"There are 1250 people\" -> \"There are [NUM] people\"`\n",
    "    - handles: `\"@xxx\" -> [HANDLE]`\n",
    "    - links: `\"www.bla.com\" [LINK]`\n",
    "    - flying punctuations (which doesn't add any meaning)\n",
    "- Outliers\n",
    "    - Unusually long or short texts\n",
    "\n",
    "Again, what to keep/remove depends if that is predictive of the task you are trying to achieve: e.g., keyword search vs text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: how long are these reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['num_chars'] = df_raw['text'].str.len()\n",
    "df_raw['num_chars'].plot.hist(bins=30, title='Number of characters in each review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the number of words in each review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if this is the right method chain to extract the number of words\n",
    "df_raw['text'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if loc[2] is actually 4 words only\n",
    "df_raw['text'].str.split().loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['num_words'] = df_raw['text'].str.split().str.len()\n",
    "df_raw['num_words'].plot.hist(bins=30, title='Number of words in each review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[['num_chars', 'num_words']].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: exponential distribution with most texts being less than 200 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall remove very long / very short texts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: we call it `df_normal` from now on to indicate the removal of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xK364R_TtFd-"
   },
   "outputs": [],
   "source": [
    "# Keep only the texts below the 90th percentile of the text lengths\n",
    "df_normal = df_raw[df_raw['num_chars'] <= df_raw['num_chars'].quantile(0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "kQHpYh8wtFeB",
    "outputId": "634ee467-407c-40b8-831a-ae9acdc45d6f"
   },
   "outputs": [],
   "source": [
    "df_normal['num_words'].plot.hist(bins=30, title='Number of words in each review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal[['num_chars', 'num_words']].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrvxgUwQtFeC"
   },
   "source": [
    "**Regular Expressions**'s key operations are:\n",
    "- Match: Determine if a string matches a pattern.\n",
    "- Find: Locate occurrences of a pattern within a string.\n",
    "- Replace: Modify a string based on a pattern.\n",
    "- Split: Divide a string into substrings using a pattern as a delimiter.\n",
    "\n",
    "See the official [Regular Expression HOWTO](https://docs.python.org/3/howto/regex.html#regex-howto) for a user-friendly introductory tutorial on Regexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLSR_n9_tFeC"
   },
   "source": [
    "üí° **Tip**: when writing regexes, we recommended you always use an interactive realtime regex tester like: https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j270uc6stFeC"
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "# Twitter handles\n",
    "user_pattern = r\"@\\w+\"\n",
    "hash_pattern = r\"#\\w+\"\n",
    "\n",
    "# start with https or start with www or end with .sa\n",
    "url_pattern = r\"https?\\S+|www.\\S+|\\S+\\.sa|\\S+\\.com\"\n",
    "\n",
    "def clean_text(x):\n",
    "    x = re.sub(user_pattern, '', x)\n",
    "    x = re.sub(hash_pattern, '', x)\n",
    "    x = re.sub(url_pattern, '', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal['text_clean'] = df_normal['text'].map(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After any deletion (cleaning), we might end up with empty texts. So let's make sure to remove that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_normal['text_clean'] = df_normal['text_clean'].str.strip()\n",
    "df_normal = df_normal[df_normal['text_clean'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZiJlJJkStFeC",
    "outputId": "9407b4f0-f9a8-49f6-8bd6-b861691652df"
   },
   "outputs": [],
   "source": [
    "# For reproducibility we want to check the same samples\n",
    "# after any operation on text\n",
    "sample_ids = df_normal.loc[45:55].index\n",
    "sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "djeZIXBEtFeC",
    "outputId": "8ba579cc-10fb-4c3f-eb58-696c867a3b94"
   },
   "outputs": [],
   "source": [
    "df_normal[['text', 'text_clean']].loc[sample_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdkC3C3jtFeD"
   },
   "source": [
    "### Normalizing Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pf6ylTuztFeD"
   },
   "source": [
    "#### Text normalization techniques\n",
    "\n",
    "- Lowercase -> `\"Car\" -> \"car\"`\n",
    "- Longataion (ÿßŸÑÿ™ÿ∑ŸàŸäŸÑ):\n",
    "    - \"ÿßŸÑŸÄŸÄÿ≥ŸÄŸÄŸÑÿßŸÖ ÿπŸÄŸÄŸÄŸÄŸÄŸÄŸÄŸÑŸäŸÉŸÖ\"\n",
    "- Extra characters denoting extra meaning (add `\"!\"` to it):\n",
    "    - `\"ÿßŸÑÿ≥ŸÑÿßÿßÿßÿßÿßÿßÿßŸÖ ÿπŸÑŸäŸÉŸÖ\" -> \"ÿßŸÑÿ≥ŸÑÿßŸÖ! ÿπŸÑŸäŸÉŸÖ\"`\n",
    "    - `\"loooooove\" -> \"love!\"`\n",
    "- Expand contractions (ÿ®ÿ≥ÿ∑ ÿßŸÑÿ±ŸÖŸàÿ≤):\n",
    "    - Static:\n",
    "        - `Don't -> Do not`\n",
    "        - `4you -> for you` (Chat Slang)\n",
    "    - Contextual: `I‚Äôd like to know how I‚Äôd done that!`\n",
    "        - `I'd -> I would`\n",
    "        - `I'd -> I had`\n",
    "        - Checkout [pycontractions](https://pypi.org/project/pycontractions/)\n",
    "- Convert Arabizi and Transliterated text into Arabic:\n",
    "    - `6areeq / tareek -> ÿ∑ÿ±ŸäŸÇ`\n",
    "    - `6a3am / ta‚Äôam -> ÿ∑ÿπÿßŸÖ`\n",
    "    - `Fondoq / fondok -> ŸÅŸÜÿØŸÇ `\n",
    "- Slice munged words\n",
    "    - `heshotwhointhewhatnow -> ['he', 'shot', 'who', 'in', 'the', 'what', 'now']`\n",
    "    - `thequickbrownfoxjumpsoverthelazydog -> ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']`\n",
    "    - Checkout: [wordninja](https://github.com/keredson/wordninja)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hNfObB-tFeD"
   },
   "source": [
    "[**PyArabic**](https://github.com/linuxscout/pyarabic) provides basic functions to manipulate Arabic letters and text, like detecting Arabic letters, Arabic letters groups and characteristics, remove diacritics etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "CXNNCYrCtFeD",
    "outputId": "ce3c3a06-3bfb-4a08-a8cf-1496043aea5b"
   },
   "outputs": [],
   "source": [
    "def normalize_text(x):\n",
    "    # remove punctuations\n",
    "    x = re.sub(r'[^\\w\\s]', '', x)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    x = re.sub(r'\\s+', ' ', x)\n",
    "\n",
    "    # remove elongations\n",
    "    x = araby.strip_tatweel(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "df_normal['text_clean'] = df_normal['text'].apply(normalize_text)\n",
    "df_normal[['text', 'text_clean']].loc[sample_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other libraries like [**tnkeeh (ÿ™ŸÜŸÇŸäÿ≠)**](https://github.com/ARBML/tnkeeh) do exist for Arabic preprocessing of common examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qGpCH1rtFeD"
   },
   "source": [
    "#### Correct Typos\n",
    "\n",
    "[**ekphrasis**](https://github.com/cbaziotis/ekphrasis): \n",
    "\n",
    "> Ekphrasis is a text processing tool, geared towards text from social networks, such as Twitter or Facebook. Ekphrasis performs tokenization, word normalization, **word segmentation** (for splitting hashtags) and **spell correction**, using word statistics from 2 big corpora (english Wikipedia, twitter - 330mil english tweets).\n",
    "\n",
    "```python\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "\n",
    "# segmenter using the word statistics from english Wikipedia\n",
    "seg_eng = Segmenter(corpus=\"english\") \n",
    "\n",
    "# segmenter using the word statistics from Twitter\n",
    "seg_tw = Segmenter(corpus=\"twitter\")\n",
    "\n",
    "words = [\"exponentialbackoff\", \"gamedev\", \"retrogaming\", \"thewatercooler\", \"panpsychism\"]\n",
    "for w in words:\n",
    "    print(w)\n",
    "    print(\"(eng):\", seg_eng.segment(w))\n",
    "    print(\"(tw):\", seg_tw.segment(w))\n",
    "    print()\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "exponentialbackoff\n",
    "(eng): exponential backoff\n",
    "(tw): exponential back off\n",
    "\n",
    "gamedev\n",
    "(eng): gamedev\n",
    "(tw): game dev\n",
    "\n",
    "retrogaming\n",
    "(eng): retrogaming\n",
    "(tw): retro gaming\n",
    "\n",
    "thewatercooler\n",
    "(eng): the water cooler\n",
    "(tw): the watercooler\n",
    "\n",
    "panpsychism\n",
    "(eng): panpsychism\n",
    "(tw): pan psych is m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spell Corrector is based on [Peter Norvig's spell-corrector](http://norvig.com/spell-correct.html). Just like the segmentation algorithm, we utilize word statistics in order to find the most probable candidate. Besides the provided statistics, you can use your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "sp = SpellCorrector(corpus=\"english\") \n",
    "print(sp.correct(\"korrect\"))\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "> correct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRCa-UFatFeD"
   },
   "source": [
    "Let's apply *Spell Correction*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "spell_checker = SpellChecker(language='ar')\n",
    "\n",
    "def correct_spelling(x):\n",
    "    misspelled = spell_checker.unknown(x.split())\n",
    "    for word in misspelled:\n",
    "        corrected = spell_checker.correction(word)\n",
    "        if corrected:\n",
    "            x = x.replace(word, corrected)\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: it actually introduced some errors, rather than fix them.\n",
    "\n",
    "- \"ŸàÿßŸÑÿ¥ÿπŸàÿ±\" -> \"ŸàÿßŸÑÿ¥ÿπŸàÿ®\"\n",
    "- \"ŸÑŸÖ ÿßÿ≥ÿ™ŸÖÿπ\" -> \"ŸÑŸÖ ÿßÿ≥ÿ™ÿ≥ÿ∫\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBIOAsVdtFeE"
   },
   "source": [
    "\n",
    "#### Stemming and Lemmatization\n",
    "\n",
    "Both simplify words..\n",
    "\n",
    "**Stemming** (ÿ™ÿ¨ÿ±ŸäÿØ), the **S**lasher, is a **S**imple, **S**peedy, **Statistical algorithm** that may result in under-stemming and over-stemming.\n",
    "\n",
    "- Result: Often produces \"stems\" that aren't real words (e.g., \"university\" $\\rightarrow$ \"univers\").\n",
    "- Performance: So fast, but inaccurate.\n",
    "\n",
    "**Lemmatization** (ÿ™ÿµÿ±ŸäŸÅ), the **L**inguist, is a **L**ookup in **L**ixicon (**Dictionary**) based on **L**anguage rules (**Grammar**).\n",
    "\n",
    "- Result: Always produces a \"lemma,\" which is a valid dictionary word (e.g., \"better\" $\\rightarrow$ \"good\").\n",
    "- Performance: Laid back, but more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "892YlTqotFeE"
   },
   "source": [
    "Comparing Qalasadi Lemmatizer vs ISRI Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "jsObQ80JtFeE",
    "outputId": "8cf90fc9-de36-4ad3-c196-69412c544b0c"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import ISRIStemmer\n",
    "import qalsadi.lemmatizer\n",
    "\n",
    "# ISRI: Arabic Stemming without a root dictionary\n",
    "stemmer = ISRIStemmer()\n",
    "lemmatizer = qalsadi.lemmatizer.Lemmatizer()\n",
    "\n",
    "words = [\"ÿßŸÑŸàŸÇÿ™\", \"ŸäŸÑÿπÿ®ŸàŸÜ\", \"ÿßŸÑÿ™ŸÑÿßŸÖÿ∞ÿ©\", \"ÿ≥Ÿäÿ£ŸÉŸÑŸàŸÜŸáÿß\"]\n",
    "\n",
    "df_stem = pd.DataFrame(words, columns=['word'])\n",
    "df_stem['stem'] = df_stem['word'].apply(lambda x: stemmer.stem(x))\n",
    "df_stem['lemma'] = df_stem['word'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "df_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT_oskyDtFeE"
   },
   "source": [
    "Giving the lemmatizer context produces a better result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vmh49RBRtFeE",
    "outputId": "b5de7ccc-be09-48ac-a7c5-08853ab8a968"
   },
   "outputs": [],
   "source": [
    "text = \"ŸäŸÑÿπÿ® ÿßŸÑÿ™ŸÑÿßŸÖÿ∞ÿ© ŸÅŸä ÿßŸÑŸàŸÇÿ™ ÿßŸÑÿ∞Ÿä ÿ≥Ÿäÿ£ŸÉŸÑŸàŸÜŸáÿß ŸÅŸäŸá\"\n",
    "lemmatizer.lemmatize_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1f32ywItFeE"
   },
   "source": [
    "Let's tokenize the text:\n",
    "\n",
    "- We will choose to stem the words (takes about 12 seconds), because we don't have much time\n",
    "- Otherwise, we would have gone with lemmatization because it is more accurate: `use_lemmatizer=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pb1TAYZntFeE"
   },
   "outputs": [],
   "source": [
    "def tokenization(x, use_lemmatizer=False):\n",
    "    if use_lemmatizer:\n",
    "        # lemmatize_text is used to be contextual (e.g., verb vs noun)\n",
    "        # it returns a list of words (tokens)\n",
    "        x = lemmatizer.lemmatize_text(x)\n",
    "    else:\n",
    "        x = araby.strip_tashkeel(x)\n",
    "        x = araby.tokenize(x)\n",
    "        x = [stemmer.stem(w) for w in x] # stemming individual words\n",
    "    return x\n",
    "\n",
    "df_normal['stemmed_tokens'] = df_normal['text_clean'].apply(lambda x: tokenization(x, use_lemmatizer=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "pc36T_-2tFeF",
    "outputId": "f4233711-4a94-4c0a-d9fe-73558a3d30d0"
   },
   "outputs": [],
   "source": [
    "# To demonstrate lemmatizer, we only apply to the first 2 rows\n",
    "pd.DataFrame({\n",
    "    'text': df_normal.loc[:2, 'text'],\n",
    "    'lemma_tokens': df_normal.loc[:2, 'text_clean'].apply(lambda x: tokenization(x, use_lemmatizer=True)),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5AnqmJ4tFeF"
   },
   "source": [
    "After cleaning, and tokenizing, now we can do stopwords removal.\n",
    "\n",
    "Note, keeping words should give us more context and hence more information for a more accurate text classification. However, **we choose to remove them to reduce computation overhead on the model**. Less features mean reduced dimensionality and hence, less computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIZXDEwLtFeF"
   },
   "source": [
    "#### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mD1rgR8UtFeF"
   },
   "source": [
    "[**Stop words**](https://en.wikipedia.org/wiki/Stop_word) are the words in a stop list (or stoplist or negative dictionary) which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are deemed insignificant.\n",
    "\n",
    "There is no single universal list of stop words used by all NLP tools, nor any agreed upon rules for identifying stop words, and indeed not all tools even use such a list. Therefore, any group of words can be chosen as the stop words for a given purpose. The \"general trend in [information retrieval] systems over time has been from standard use of quite large stop lists (200‚Äì300 terms) to very small stop lists (7‚Äì12 terms) to no stop list whatsoever\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_3ZWfICtFeF",
    "outputId": "1d6133b8-cd71-4b5c-a675-72eb88ffe0ba"
   },
   "outputs": [],
   "source": [
    "stopwords_ar = nltk.corpus.stopwords.words('arabic')\n",
    "\n",
    "print(len(stopwords_ar)) # how many stop words?\n",
    "print(stopwords_ar[:10]) # first 10 stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6TCxQThtFeF"
   },
   "outputs": [],
   "source": [
    "stopwords_ar.append('ÿ¨ŸÖŸäÿπ') # add a custom stopword\n",
    "stopwords_ar.append('ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ Ÿàÿ±ÿ≠ŸÖÿ© ÿßŸÑŸÑŸá Ÿàÿ®ÿ±ŸÉÿßÿ™Ÿá') # add a custom phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDDORBDutFeF",
    "outputId": "401346fe-9182-488a-c8af-3d13a7e990bd"
   },
   "outputs": [],
   "source": [
    "print(len(stopwords_ar)) # should be more now\n",
    "print(stopwords_ar[-5:]) # last 5 stop words now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7aiX6B_tFeG"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(xs):\n",
    "    return [w for w in xs if w not in stopwords_ar]\n",
    "\n",
    "df_normal['stemmed_tokens'] = df_normal['stemmed_tokens'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "tiw6dtEDtFeG",
    "outputId": "b159a154-bb4c-4c1d-c30e-d726b9996b1b"
   },
   "outputs": [],
   "source": [
    "df_normal.loc[sample_ids, ['text_clean', 'stemmed_tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSsDk4dVtFeG"
   },
   "source": [
    "### Morphological Tokenization  (Optional)\n",
    "\n",
    "Morphological tokenization whereby Arabic words are split into component prefixes, stems, and suffixes.\n",
    "\n",
    "- Input Tokens: `['ŸÅÿ™ŸÜŸÅÿ≥ÿ™', 'ÿßŸÑÿµÿπÿØÿßÿ°']`\n",
    "- Output: split morphologically and apply diacritics: `['ŸÅŸé+', 'ÿ™ŸéŸÜŸéŸÅŸëŸéÿ≥Ÿíÿ™Ÿè', 'ÿßŸÑ+', 'ÿµŸèÿπŸéÿØÿßÿ°Ÿé']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools) is suite of Arabic natural language processing tools developed by the CAMeL Lab at New York University Abu Dhabi.\n",
    "\n",
    "See the [CAMeL Tools Guided Tour](https://colab.research.google.com/drive/1Y3qCbD6Gw1KEw-lixQx1rI6WlyWnrnDS?usp=sharing#scrollTo=KmeKhpPVsI69); Morphological Tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install CAMeL Tools Data\n",
    "\n",
    "CAMeL Tools requires additional data files for morphological analysis. We install the \"light\" version for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install camel-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install just the datasets for morphology and MLE disambiguation only\n",
    "# !camel_data -i light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AG1i4G-itFeG",
    "outputId": "9c79014b-8888-43db-d979-8137d8bf48cc"
   },
   "outputs": [],
   "source": [
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "\n",
    "# The tokenizer expects pre-tokenized text\n",
    "sentence = simple_word_tokenize('ŸÅÿ™ŸÜŸÅÿ≥ÿ™ ÿßŸÑÿµÿπÿØÿßÿ°')\n",
    "print(sentence)\n",
    "\n",
    "# Load a pretrained disambiguator to use with a tokenizer\n",
    "mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "\n",
    "# We can output diacritized tokens by setting `diac=True`\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok',\n",
    "                                   split=True, # split affixes\n",
    "                                   diac=True,  # diacritize\n",
    "                                   )\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVG51UNztFeG",
    "outputId": "9f2faac1-9587-4002-88d0-89e621b91f8e"
   },
   "outputs": [],
   "source": [
    "def tokenize_morph(x):\n",
    "    x = simple_word_tokenize(x)\n",
    "    return tokenizer.tokenize(x)\n",
    "\n",
    "# Due to processing time, we only demonstrate it\n",
    "print(df_normal.loc[:5, 'text_clean'].apply(tokenize_morph).apply(lambda xs: ' '.join(xs)).str.cat(sep='\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFFevy3atFeG"
   },
   "source": [
    "**Note**: to make the tutorial light and quick, we will not use **morphological tokenization** (it is computationally heavy). However, we think it is much more accurate to do such tokenization than simple *lemmatizers* or *stemmers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal.to_csv('ar_reviews_100k_cleaned.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Data acquisition** from external sources (like Kaggle) requires proper authentication and data extraction techniques.\n",
    "\n",
    "- **Exploratory Data Analysis (EDA)** is essential before preprocessing to understand:\n",
    "  - Class distribution (balanced vs. imbalanced datasets)\n",
    "  - Text length distributions (identifying outliers)\n",
    "  - Language detection (ensuring expected language)\n",
    "  - Vocabulary characteristics (size, frequency patterns)\n",
    "  - Class-specific word patterns\n",
    "\n",
    "- **Data splitting** must occur before preprocessing to prevent data leakage from test set to training set.\n",
    "\n",
    "- **Preprocessing order matters critically**:\n",
    "  1. Extract structured information (URLs, handles, hashtags)\n",
    "  2. Normalize text (remove elongations, diacritics)\n",
    "  3. Tokenize\n",
    "  4. Stem/Lemmatize\n",
    "  5. Remove stop words last\n",
    " \n",
    "- **Preprocessing trade-offs** exist between:\n",
    "  - Speed vs. accuracy (stemming vs. lemmatization)\n",
    "  - Simple models needs simple features and vice-versa (stop words removal)\n",
    "  - Vocabulary size vs. computational cost\n",
    "\n",
    "- Understanding your data through EDA guides preprocessing decisions and helps build effective NLP pipelines for Arabic text classification."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
